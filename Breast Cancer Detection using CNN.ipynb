{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569d6373-14d3-4624-9405-dbbe26808b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy opencv-python pillow tensorflow keras imutils scikit-learn matplotlib kagglehub matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff43317-e203-4f84-bbe4-29e4aeac65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"paultimothymooney/breast-histopathology-images\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e39a54-15cb-478c-b5b5-705952f44332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (SeparableConv2D, MaxPooling2D, Flatten,\n",
    "                                     Dropout, Dense, Activation, BatchNormalization)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configuration\n",
    "INPUT_DATASET = r\"C:\\Users\\DELL\\.cache\\kagglehub\\datasets\\paultimothymooney\\breast-histopathology-images\\versions\\1\"\n",
    "BASE_PATH = r\"\"\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, \"training\")\n",
    "VAL_PATH = os.path.join(BASE_PATH, \"validation\")\n",
    "TEST_PATH = os.path.join(BASE_PATH, \"testing\")\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "IMG_DIMS = (48, 48)\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "INIT_LR = 1e-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e2a8db-753c-482c-b076-44fadb79d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "\n",
    "def build_dataset_structure():\n",
    "    image_paths = list(paths.list_images(INPUT_DATASET))\n",
    "    random.seed(7)\n",
    "    random.shuffle(image_paths)\n",
    "\n",
    "    i = int(len(image_paths) * TRAIN_SPLIT)\n",
    "    train_paths = image_paths[:i]\n",
    "    test_paths = image_paths[i:]\n",
    "\n",
    "    j = int(len(train_paths) * VAL_SPLIT)\n",
    "    val_paths = train_paths[:j]\n",
    "    train_paths = train_paths[j:]\n",
    "\n",
    "    datasets = [(\"training\", train_paths, TRAIN_PATH),\n",
    "                (\"validation\", val_paths, VAL_PATH),\n",
    "                (\"testing\", test_paths, TEST_PATH)]\n",
    "\n",
    "    for set_type, paths_list, base_path in datasets:\n",
    "        print(f\"Building {set_type} set\")\n",
    "        for path in paths_list:\n",
    "            label = path.split(os.path.sep)[-1][-5:-4]\n",
    "            label_path = os.path.join(base_path, label)\n",
    "            os.makedirs(label_path, exist_ok=True)\n",
    "            shutil.copy2(path, os.path.join(label_path, os.path.basename(path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf344c19-ac54-4625-9e8d-aef999d931f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=(48, 48, 3), classes=2):\n",
    "    model = Sequential()\n",
    "    channel_dim = -1\n",
    "\n",
    "    model.add(SeparableConv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=channel_dim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=channel_dim))\n",
    "    model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=channel_dim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=channel_dim))\n",
    "    model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=channel_dim))\n",
    "    model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=channel_dim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "639ebd32-687d-4437-b32f-f22c66d76157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    train_aug = ImageDataGenerator(\n",
    "        rescale=1 / 255.0,\n",
    "        rotation_range=20,\n",
    "        zoom_range=0.05,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.05,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    val_aug = ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "    train_gen = train_aug.flow_from_directory(\n",
    "        TRAIN_PATH,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=IMG_DIMS,\n",
    "        color_mode=\"rgb\",\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    val_gen = val_aug.flow_from_directory(\n",
    "        VAL_PATH,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=IMG_DIMS,\n",
    "        color_mode=\"rgb\",\n",
    "        shuffle=False,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    test_gen = val_aug.flow_from_directory(\n",
    "        TEST_PATH,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=IMG_DIMS,\n",
    "        color_mode=\"rgb\",\n",
    "        shuffle=False,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Class Weights\n",
    "    labels = train_gen.classes\n",
    "    class_totals = np.bincount(labels)\n",
    "    class_weight = {i: max(class_totals) / class_totals[i] for i in range(NUM_CLASSES)}\n",
    "\n",
    "    model = build_model(input_shape=(48, 48, 3), classes=NUM_CLASSES)\n",
    "    opt = Adam(learning_rate=INIT_LR)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "    H = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=train_gen.samples // BATCH_SIZE,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=val_gen.samples // BATCH_SIZE,\n",
    "        class_weight=class_weight,\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    test_gen.reset()\n",
    "    preds = model.predict(test_gen, steps=(test_gen.samples // BATCH_SIZE) + 1)\n",
    "    pred_indices = np.argmax(preds, axis=1)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_gen.classes, pred_indices, target_names=test_gen.class_indices.keys()))\n",
    "\n",
    "    cm = confusion_matrix(test_gen.classes, pred_indices)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    total = cm.sum()\n",
    "    accuracy = np.trace(cm) / total\n",
    "    specificity = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "    sensitivity = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e05533-50f4-422b-983e-3b5476587fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set\n",
      "Building validation set\n",
      "Building testing set\n",
      "Found 255815 images belonging to 2 classes.\n",
      "Found 42660 images belonging to 2 classes.\n",
      "Found 99906 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_separable_conv.py:104: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m2252/7994\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11:47\u001b[0m 750ms/step - accuracy: 0.7682 - loss: 0.7855"
     ]
    }
   ],
   "source": [
    "build_dataset_structure()\n",
    "train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50940e-6091-40e2-af54-8a6c9a03ef51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
